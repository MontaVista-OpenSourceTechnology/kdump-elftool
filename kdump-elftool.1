.TH kdump-elftool 1 06/02/01  "Kdump dump handling tool"

.SH NAME
kdump-elftool \- Kdump dump handling tool

.SH SYNOPSIS
.B kdump-elftool
[\-\-help] topelf [\--help]
[\-\-oldmem|\-i <oldmem>]
[\-\-intype|-I oldmem|qemu|makedumpfile|kdump]
[\-\-outfile|-o <output file>]
[\-\-vmcore|-v <vmcore>]
[\-\-elfclass|-c 32|64]
[\-\-level|-l all|inuse|user|cache|kernel]
[\-\-extravminfo|-e <vminfofile>]
[\-\-debug|-d]
[\-\-m32|\-4] [\-\-m64|\-8]

.B kdump-elftool
[\-\-help] tovelf [\--help]
[\-\-infile|\-i <oldmem or pelf>]
[\-\-outfile|-o <output file>]
[\-\-vmcore|-v <vmcore>]
[\-\-intype|-I oldmem|pelf|qemu|makedumpfile|kdump]
[\-\-physpgd|-P <pgd phys address>]
[\-\-elfclass|-c 32|64]
[\-\-level|-l all|inuse|user|cache|kernel]
[\-\-vmlinux|-m <vmlinux>]
[\-\-extravminfo|-e <vminfofile>]
[\-\-procthreads|-p]
[\-\-debug|-d]
[\-\-m32|\-4] [\-\-m64|\-8]

.B kdump-elftool
[\-\-help] addrandoff [\--help]
[\-\-vmcore|-v <vmcore>]
[\-\-vmlinux|-m <vmlinux>]
[\-\-extravminfo|-e <vminfofile>]

.B kdump-elftool
[\-\-help] makedyn [\--help]
<vmlinux> [<vmlinux> [...]]

.B kdump-elftool
[\-\-help] dumpmem [\--help]
[\-\-infile|\-i <oldmem or pelf>]
[\-\-outfile|-o <output file>]
[\-\-vmcore|-v <vmcore>]
[\-\-intype|-I oldmem|pelf]
[\-\-is_physical|-p]
[\-\-extravminfo|-e <vminfofile>]
<address> <size>

.B kdump-elftool
[\-\-help] virttophys [\--help]
[\-\-infile|\-i <oldmem or pelf>]
[\-\-vmcore|-v <vmcore>]
[\-\-intype|-I oldmem|pelf]
[\-\-physpgd|-P <pgd phys address>]
[\-\-extravminfo|-e <vminfofile>]
[\-\-debug|-d]
<address>

.SH DESCRIPTION
The
.BR kdump-elftool
program extracts kernel coredumps from the target and processes those
coredumps into something gdb can understand.
.PP
In a kdump crash kernel, there are two files that let you extract
information about the kernel that just crashed: /dev/mem and
/proc/vmcore.  /dev/mem is a raw file holding an image of the physical
memory of the system, including the previous host's memory.  Normally
this is not used, because /proc/vmcore also has a map of the memory
and the data is pulled from there.
/proc/vmcore is a kernel coredump plus some extra information.  (See
.BR RATONALE
for why this coredump is not sufficient.)

.BR kdump-elftool
deals with two other types of files, both elf files.  Physical memory
elf files (pelf) and virtual memory elf files (velf).  A pelf file
holds a physical memory coredump, basically what comes out of
/proc/vmcore, but with all the unnecessary pages removed.  Note that
.BR kdump-elftool
will get the physical memory ranges for the old kernel out of
/proc/vmcore, if available, and only store the necessary ranges.

Note that you can just copy /proc/vmcore and use that as a pelf file
if you don't want to have kdump-elftool on the target.  But it will
contain all of memory, so it will be a very large file.

A velf file holds a virtual memory coredump, this is in a format that
gdb can load load and use.  gdb needs the virtual kernel addresses to
work.

.BR "kdump-elftool topelf -o <pelf file>"
will process /proc/vmcore into a pelf file.  This is
generally preferred on a target system, pelf files are usually smaller
than velf files and they are quicker to process.  Plus a velf file may
not have all of physical memory, in case you are in a desperate
situation and need to get to userland memory.  Note that the output
will go to standard output if you don't specify -o.

.BR "kdump-elftool tovelf -i <pelf file> -o <velf file>"
will process the pelf file into a velf file.

.BR "kdump-elftool tovelf -I oldmem -o <velf file>"
will process /proc/vmcore directly into a velf file.

For
.BR gdb
to properly interpret kernels with a random base address, it has to be
told the load offset.  It has to have the original
.BR <vmlinux>
file to do this, then it can compare symbol offsets to symbol addresses
it has access to in the dump file.  Adding the
.BR "--vmlinux <vmlinux>"
option to the
.BR tovelf
subcommand will do this, or you can add it later to an existing virtual
memory coredump with the
.BR "kdump-elftool addrandoff -m <vmlinux> -v <vmcore>"
command.

As an additional problem for debugging kernels with a random base address,
the
.BR vmlinux
compile for most architectures is marked as a regular executable, but
.BR gdb
requires that it be a relocatable executable to handle the load offset.
The command
.BR "kdump-elftool makedyn <vmlinux>"
will convert a vmlinux to a relocatable executable command for you.

.BR kdump-elftool
will also dump ranges of memory from the various input types it
supports using the
.BR dumpmem
subcommand.  The parameters all work as
before, with the addition of -p, which specifies the address as
physical.  Otherwise the address is assumed to be virtual.

.BR kdump-elftool
can also convert logical addresses to physical addresses using the
.BR virttophys
subcommand.  This only works on physical memory files.

.SH OPTIONS
.TP
.I "\-\-help"
Output help.
.TP
.I "\-\-oldmem|\-i <oldmem>"
Set the location of the oldmem file.  The default is unused, the memory is
pulled from the vmcore.
.TP
.I "\-\-outfile|-o <output file>"
Set the output file, output will go to stdout if this is not supplied.
.TP
.I "\-\-vmcore|-v <vmcore>"
Set the location of the vmcore file, /proc/vmcore by default.
.TP
.I "\-\-vmlinux|-m <vmlinux>"
The vmlinux file for the core dump being processed.  Require to add the
load offset for a randomized base kernel.
.TP
.I "\-\-infile|\-i <oldmem or pelf>"
Set the location of the input file for converting to velf.  This can
either be a raw memory file (like /dev/mem) if
.BR \-I oldmem
is specified.  Normally this is not used with oldmem, it will pull the
data from the vmcore if this is not specified.  If
.BR \-I oldmem
is not given, then this parameter is required and should be a physical
elf file.
.TP
.I "\-\-intype|-I oldmem|pelf|qemu|makedumpfile|kdump"
Set the input file time for conversion to velf.  oldmem sets it to use
/proc/vmcore (and /dev/mem if specified).  pelf sets it to use a file
that was processed with this program with topelf.  qemu sets it to use
a QEMU vmdump file.  See the section QEMU VMDUMP for details.
makedumpfile and kdump sets it to use a dump generated by the
makedumpfile tool, see the section MAKEDUMPFILE/KDUMP VMDUMP for details.
.TP
.I "\-\-physpgd|-P <pgd phys address>"
Set the physical address of the pgd pointer.  This is read from the
input file, but if it is missing or wrong it can be overridden.  This
can also be used to create a coredump of a process if you have the
process' physical page directory address.  That can be obtained with
gdb.  It will only dump the process memory that is present, so if
memory is not yet paged in or has been paged out, it will not be
present.
.TP
.I "\-\-elfclass|-c 32|64"
All physical memory coredumps are 64-bit because many 32-bit
architectures can map physical addresses greater than 32-bits (like
x86 PAE).  For architectures, like MIPS, where the kernel may be 32 or
64 bits, you have to set the class to 32 bits if you have a 32-bit
kernel (or are generating a coredump for a userland process that is
32-bits).  x86_64 and i386 are distinct architectures, so you won't
have a problem with the kernel, but you might have the same issue for
userland.
.TP
.I "\-\-level|-l all|inuse|user|cache|kernel"
Select which pages of memory to put into the dump.  The default level
is kernel, which will dump pages used by the kernel.  This is the most
useful, in general, for coredump analysis since free, cache, and user
pages aren't terribly relevant to analyzing the kernel.  The cache
level will dump kernel and cache pages.  The user level will dump
kernel and user pages.  The inuse level will dump all pages that are
not free, and all obviously just dumps all pages.
.TP
.I "\-\-extravminfo|-e <vminfofile>"
Add extra symbol and offset information that was not available in the
kernel dump.  This will override existing information in the kernel
dump if there are duplicates.
.TP
.I "\-\-procthreads|-p"
Convert processed in the kernel into threads that gdb can understand.
See "PROCESSES TO GDB THREADS" below for more details.
.TP
.I "\-\-debug|-d"
Dump debug information.  Generally only useful if you are debugging
kdump-elftool itself.
.TP
.I "\-\-m32|\-4"
Set the machine type to a 32-bit machine.  Used by QEMU processing,
see QEMU VDUMP below for details.
.TP
.I "\-\-m64|\-8"
Set the machine type to a 64-bit machine.  Used by QEMU processing,
see QEMU VDUMP below for details.

.SH RATIONALE
Why not just use /proc/vmcore, you ask?  The /proc/vmcore file
generated by the kernel does not have all the virtual memory sections
available.  Particularly, vmalloc memory and vmemmap are not
available, and all of physical memory may not be present.  Since
modules reside in vmalloc and some systems have the pages array in
vmemmap, it's almost impossible to use gdb on the standard kernel
coredump.

.SH PROCESSES TO GDB THREADS
kdump-elftool can convert every kernel process into a thread that gdb
can use.  You generally have to use the macro from kdump_gdbinit
named thread_vminfo to get the extra vminfo file, pass it in to
kdump-elftool with
.I \-\-extravminfo <file>
with that file to get the proper symbols.  Then use the
.I \-\-procthreads
option to do the conversion.  The procedure would generally be

.RS 4
Get a physical dump of the kernel.  Physical dumps are generally recommended
for the target, they are smaller and can be easily processed on the host.

<Convert it to a normal virtual dump.  This is required because if the
 kernel is relocated, you need relocated symbols.>

kdump-elftool tovelf -I pelf -i pdump -m vmlinux -o vdump

gdb vmlinux vdump

source kdump_gdbinit

thread_vminfo_<arch>  # <arch> is either mips, arm, i386, or x86_64

<save the output to a file name thread_vminfo and quit gdb>

kdump-elftool tovelf -I pelf -i pdump -m vmlinux -o vdump -e thread_vminfo -p

gdb vmlinux vdump
.RE

And you should see all the process as threads.

Note that you do
.I NOT
get the userland traceback here.  You get the
kernel side of the traceback.

.SH PROCESSES TO GDB THREADS EXTRAS FOR X86_64
Unfortunately, x86_64 kernels before 4.9 do not provide all the
information you need to properly generate threads for each process.
Two additional pieces of information are required: The context switch
point and the value of the BP register at context switch.

Kernels 4.9 and later do not need this extra handling.

The kernel does not store the value of the BP register at context
switch, and that would slow down context switches a little so it's
frowned upon.  So to get this, you must calculate the frame size
of __schedule and set that in thread_vminfo file.

The first thing to do, though, is to find the location of the
__switch_to call.  It's normally in the __schedule function, but
it may be in another function called from __schedule, like
__schedule_nobkl.

If you have the x86 patch in the kernel-patches directory applied to
your kernel, then it should contain the code to add the context switch
point.  If you don't then all is not lost, you can find it.  First
convert your physical coredump into a normal virtual coredump and load
it in to gdb.  (You have to actually do this with a coredump because
the kernel can be relocated and you need to know the relocated symbol
point.)  Then do:

.RS 4
x/10i __schedule
.RE

Just keep hitting return until you find a call to __switch_to, like:

.RS 4
0xffffffff81744513 <__schedule+643>:	mov    %rsp,0x408(%rdi)
.br
0xffffffff8174451a <__schedule+650>:	mov    0x408(%rsi),%rsp
.br
0xffffffff81744521 <__schedule+657>:	callq  0xffffffff810013c0 <__switch_to>
.br
0xffffffff81744526 <__schedule+662>:	mov    %gs:0x9900,%rsi
.RE

Get the location of the instruction right after the callq and add:

.RS 4
SYMBOL(__thread_sleep_point)=ffffffff81744526
.RE

to your thread_vminfo file.  Some versions of gdb might not
continue the action after hitting return, and the call to
__switch_to can be way down there.  So it might be best to
do:

.RS 4
x/1000i __schedule
.RE

and hunt until you find it.  It might be easier to do

.RS 4
set height 0
.RE

and do the "x/1000i", and then cut and paste the output to a file so
you can use an editor to search for __switch_to and the operations on
rsp that are talked about later.

Note that __schedule may not call __switch_to.  You will have to
find the function that __schedule calls that calls __switch_to,
(like __schedule_nobkl) and do the procedure in that function.

Now that you know where __switch_to is called from, you need to
calculate the amount of stack used in that function at the point where
__switch_to is called, so the program can properly calculate the value
of BP.  To calculate it, load the vmlinux file into gdb (as before)
and do

.RS 4
x/20i __schedule
.RE

(or __schedule_nobkl, or whatever function contains __switch_to).
You should see something like:

.RS 4
0xffffffff81744290 <__schedule>:	push   %rbp
.br
0xffffffff81744291 <__schedule+1>:	mov    $0xdf40,%rax
.br
0xffffffff81744298 <__schedule+8>:	mov    %gs:0x9908,%rdx
.br
0xffffffff817442a1 <__schedule+17>:	mov    %rsp,%rbp
.br
0xffffffff817442a4 <__schedule+20>:	push   %r15
.br
0xffffffff817442a6 <__schedule+22>:	push   %r14
.br
0xffffffff817442a8 <__schedule+24>:	push   %r13
.br
0xffffffff817442aa <__schedule+26>:	push   %r12
.br
0xffffffff817442ac <__schedule+28>:	push   %rbx
.br
0xffffffff817442ad <__schedule+29>:	sub    $0x48,%rsp
.RE

You need to count how much the stack is decremented here after the
push of %rbp.  On x86_64, each push is 8 bytes, then you see a direct
subtraction from %rsp.  So there are five pushes and then another 72
(0x48) bytes subtracted, so this is 72 + (5 * 8), or 112 bytes.  Then
set the

.RS 4
SIZE(context_switch_frame)=1
.RE

in your thread_vminfo file from one to the value you calculate.  Be
careful, there may be sneaky pushes around __switch_to that you may
have to account for, too.  So keep looking and scanning for pushes,
pops, and modifications to the rsp register.  If you don't calculate
this correctly, the backtraces won't make any sense.

.SH QEMU VMDUMP
kdump-elftool can process QEMU vmdump file from i386 and x86_64
systems.  It is, unfortunately, somewhat complicated because that file
doesn't have any direct access to vminfo information that is used for
the processing of the dump.  However, it is possible to work around
this.

To do this, load the vmlinux file into gdb, source kdump_gdbinit,
and run vminfo_qemu_base.  This will print out the vminfo information
required to get kdump-elftool started.  Something like:
.RS 4
gdb vmlinux
.br
GNU gdb (Ubuntu 8.1-0ubuntu3) 8.1.0.20180409-git
.br
....
.br
(gdb) source ../kdump_gdbinit
.br
(gdb) vminfo_qemu_base
.br
SYMBOL(vmcoreinfo_data)=0xffffffff819dcf40
.br
OSRELEASE=2.6.32.71+
.br
SIZE(list_head)=16
.br
OFFSET(list_head.next)=0
.br
OFFSET(list_head.prev)=8
.br
SYMBOL(_stext)=ffffffff8100c000
.br
SYMBOL(idt_table)=0xffffffff81985000
.br
SYMBOL(per_cpu__current_task)=0xe880
.br
PAGESIZE=4096
.RE

Then save that output in a file, say vminfo.  Then run the command
.RS 4
kdump-elftool tovelf -I qemu -i qemu_vmcore -e vminfo --m64 -o vcore
.RE
You have to specify whether qemu was a 32 or 64-bit machine, because
there's no way to tell from the vmcore file, and you can run a 32-bit
kernel in a 64-bit VM.

You can use this with
.I \-\-procthreads
but you will need to append the thread processing vmcore info to the
vminfo file.

Note that this requires that the kernel be compiled with kernel core
dump support, even if it isn't configured, because it pulls the kernel
vminfo data from an internal data structure.  If you don't have that
there is a vminfo_qemu_all command in kdump_gdbinit that dumps all
that it can.  There are some length fields that are not filled in,
though.  You will need to fill those values in yourself.  They are
marked with "??".

.SH MAKEDUMPFILE/KDUMP VMDUMP
If you have a dump taken with makedumpfile, kdump-elftool might be
able to read it.  Currently it only works with x86_64, but adding
new machines should be simple.

These some in two types, a kdump file and a makedumpfile, depending on
the parameters to makedumpfile.  If you "hexdump -C" the file, a kdump
file starts with "KDUMP" and a makedumpfile starts with
"makedumpfile".

You might have to add a vminfo file if the dump file does not have
vmcoreinfo data in it, the tool will tell you if that is the case.
This is the same procedure as getting the vminfo file in the
QEMU VMDUMP section.

.SH KERNEL MODULES
To make module symbols available to gdb, you have to load the module
symbols.  To do this, use lsmod to dump a list of the modules, get the
"Base Addr" for the module you are interested in, and run the command:
.RS 4
add-symbol-file <module .o> <Base Addr>
.RE
Note that you use the .o file, not the .ko file, for the module, and
the module must match the kernel, of course.

.SH FILES
/dev/mem, /proc/vmcore

See Documentation/kdump/kdump.txt in the Linux kernel for more details.

.SH "SEE ALSO"
kexec(8)

.SH "KNOWN PROBLEMS"
This is still fairly primitive and doesn't support all architectures.

Process to gdb thread processing only works for MIPS, ARM, and X86_64.

.SH AUTHOR
.PP
Corey Minyard <minyard@acm.org>
